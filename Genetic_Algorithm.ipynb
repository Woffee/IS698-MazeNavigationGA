{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Genetic-Algorithm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL3lqte5DC6U"
      },
      "source": [
        "#**Genetic Algorithm Notebook**\n",
        "\n",
        "This code is an except from David Ha's ESTool available at https://github.com/hardmaru/estool\n",
        "\n",
        "Assembled by:\n",
        "*   Amy K Hoover <ahoover@njit.edu>, 11/2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlUn-5bnA5jC"
      },
      "source": [
        "## Genetic Algorithm Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYRvVXN_-E8A"
      },
      "source": [
        "NPARAMS = 100        # make this a 100-dimensinal problem.\n",
        "NPOPULATION = 101    # use population size of 101.\n",
        "MAX_ITERATION = 4000 # run each solver for 5000 generations."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Wa1OuE-E7y"
      },
      "source": [
        "## Genetic Algorithm Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oIJEEf2-spb"
      },
      "source": [
        "class SimpleGA:\n",
        "  '''Simple Genetic Algorithm.'''\n",
        "  def __init__(self, num_params,      # number of model parameters\n",
        "               sigma_init=0.1,        # initial standard deviation\n",
        "               sigma_decay=0.999,     # anneal standard deviation\n",
        "               sigma_limit=0.01,      # stop annealing if less than this\n",
        "               popsize=256,           # population size\n",
        "               elite_ratio=0.1,       # percentage of the elites\n",
        "               forget_best=False,     # forget the historical best elites\n",
        "               weight_decay=0.01,     # weight decay coefficient\n",
        "              ):\n",
        "\n",
        "    self.num_params = num_params\n",
        "    self.sigma_init = sigma_init\n",
        "    self.sigma_decay = sigma_decay\n",
        "    self.sigma_limit = sigma_limit\n",
        "    self.popsize = popsize\n",
        "\n",
        "    self.elite_ratio = elite_ratio\n",
        "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
        "\n",
        "    self.sigma = self.sigma_init\n",
        "    self.elite_params = np.zeros((self.elite_popsize, self.num_params))\n",
        "    self.elite_rewards = np.zeros(self.elite_popsize)\n",
        "    self.best_param = np.zeros(self.num_params)\n",
        "    self.best_reward = 0\n",
        "    self.first_iteration = True\n",
        "    self.forget_best = forget_best\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def rms_stdev(self):\n",
        "    return self.sigma # same sigma for all parameters.\n",
        "\n",
        "  def ask(self):\n",
        "    '''returns a list of parameters'''\n",
        "    self.epsilon = np.random.randn(self.popsize, self.num_params) * self.sigma\n",
        "    solutions = []\n",
        "    \n",
        "    def mate(a, b):\n",
        "      c = np.copy(a)\n",
        "      idx = np.where(np.random.rand((c.size)) > 0.5)\n",
        "      c[idx] = b[idx]\n",
        "      return c\n",
        "    \n",
        "    elite_range = range(self.elite_popsize)\n",
        "    for i in range(self.popsize):\n",
        "      idx_a = np.random.choice(elite_range)\n",
        "      idx_b = np.random.choice(elite_range)\n",
        "      child_params = mate(self.elite_params[idx_a], self.elite_params[idx_b])\n",
        "      solutions.append(child_params + self.epsilon[i])\n",
        "\n",
        "    solutions = np.array(solutions)\n",
        "    self.solutions = solutions\n",
        "\n",
        "    return solutions\n",
        "\n",
        "  def tell(self, reward_table_result):\n",
        "    # input must be a numpy float array\n",
        "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
        "\n",
        "    reward_table = np.array(reward_table_result)\n",
        "    \n",
        "    if self.weight_decay > 0:\n",
        "      l2_decay = compute_weight_decay(self.weight_decay, self.solutions)\n",
        "      reward_table += l2_decay\n",
        "\n",
        "    if self.forget_best or self.first_iteration:\n",
        "      reward = reward_table\n",
        "      solution = self.solutions\n",
        "    else:\n",
        "      reward = np.concatenate([reward_table, self.elite_rewards])\n",
        "      solution = np.concatenate([self.solutions, self.elite_params])\n",
        "\n",
        "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
        "\n",
        "    self.elite_rewards = reward[idx]\n",
        "    self.elite_params = solution[idx]\n",
        "\n",
        "    self.curr_best_reward = self.elite_rewards[0]\n",
        "    \n",
        "    if self.first_iteration or (self.curr_best_reward > self.best_reward):\n",
        "      self.first_iteration = False\n",
        "      self.best_reward = self.elite_rewards[0]\n",
        "      self.best_param = np.copy(self.elite_params[0])\n",
        "\n",
        "    if (self.sigma > self.sigma_limit):\n",
        "      self.sigma *= self.sigma_decay\n",
        "\n",
        "  def current_param(self):\n",
        "    return self.elite_params[0]\n",
        "\n",
        "  def set_mu(self, mu):\n",
        "    pass\n",
        "\n",
        "  def best_param(self):\n",
        "    return self.best_param\n",
        "\n",
        "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
        "    return (self.best_param, self.best_reward, self.curr_best_reward, self.sigma)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyz3ePmy-E70"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMCqwrVEBIVA"
      },
      "source": [
        "## Fitness Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nceKdSnv-E77"
      },
      "source": [
        "# from https://github.com/CMA-ES/pycma/blob/master/cma/fitness_functions.py\n",
        "def rastrigin(x):\n",
        "  \"\"\"Rastrigin test objective function, shifted by 10. units away from origin\"\"\"\n",
        "  x = np.copy(x)\n",
        "  x -= 10.0\n",
        "  if not np.isscalar(x[0]):\n",
        "    N = len(x[0])\n",
        "    return -np.array([10 * N + sum(xi**2 - 10 * np.cos(2 * np.pi * xi)) for xi in x])\n",
        "  N = len(x)\n",
        "  ## Note: We are evaluating individuals and returning this value as a fitness\n",
        "  return -(10 * N + sum(x**2 - 10 * np.cos(2 * np.pi * x)))\n",
        "\n",
        "# TODO: set our evaluation function to the definition of runSimulation\n",
        "fit_func = rastrigin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_W2RyCVBCqN"
      },
      "source": [
        "## Simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1op0Mbdv-E8C"
      },
      "source": [
        "# Main Genetic Algorithm Loop - defines a function to use solver to solve fit_func\n",
        "# This code is equivalent to def runSimulation\n",
        "def test_solver(solver):\n",
        "  history = []\n",
        "\n",
        "  for j in range(MAX_ITERATION):\n",
        "    solutions = solver.ask()\n",
        "    fitness_list = np.zeros(solver.popsize)\n",
        "\n",
        "    for i in range(solver.popsize):\n",
        "      fitness_list[i] = fit_func(solutions[i])\n",
        "\n",
        "    solver.tell(fitness_list)\n",
        "    result = solver.result() # first element is the best solution, second element is the best fitness\n",
        "    history.append(result[1])\n",
        "\n",
        "    if (j+1) % 100 == 0:\n",
        "      print(\"fitness at iteration\", (j+1), result[1])\n",
        "\n",
        "  print(\"local optimum discovered by solver:\\n\", result[0])\n",
        "  print(\"fitness score at this local optimum:\", result[1])\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rmngbsmCQ4u"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEtg6EV7Vf3Q"
      },
      "source": [
        "This code is similar to the cell under Run in 'Maze Navigation.' In it, we define a search algorithm called SimpleGA, run the search, and save the results in ga_history. We save the results to this variable so that we can plot fitness over time. In the Maze Navigation code, we did not save our results but instead output simulation code directly to the ipynb standard out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2IOnC8h-E8O"
      },
      "source": [
        "import numpy as np\n",
        "# defines genetic algorithm solver\n",
        "ga = SimpleGA(NPARAMS,                # number of model parameters\n",
        "               sigma_init=0.5,        # initial standard deviation\n",
        "               popsize=NPOPULATION,   # population size\n",
        "               elite_ratio=0.1,       # percentage of the elites\n",
        "               forget_best=False,     # forget the historical best elites\n",
        "               weight_decay=0.00,     # weight decay coefficient\n",
        "              )\n",
        "ga_history = test_solver(ga)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ef85cKaBhsZ"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOuPYOoU-E8E"
      },
      "source": [
        "x = np.zeros(NPARAMS) # 100-dimensional problem\n",
        "print(\"This is F(0):\")\n",
        "print(rastrigin(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gAwHS2s-E8H"
      },
      "source": [
        "x = np.ones(NPARAMS)*10. # 100-dimensional problem\n",
        "print(rastrigin(x))\n",
        "print(\"global optimum point:\\n\", x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBe7tWXl-E8o"
      },
      "source": [
        "# Create a new figure of size 8x6 points, using 100 dots per inch\n",
        "best_history = [0] * MAX_ITERATION\n",
        "plt.figure(figsize=(16,8), dpi=150)\n",
        "\n",
        "optimum_line, = plt.plot(best_history, color=\"black\", linewidth=0.5, linestyle=\"-.\", label='Global Optimum')\n",
        "ga_line, = plt.plot(ga_history, color=\"green\", linewidth=1.0, linestyle=\"-\", label='GA')\n",
        "#oes_line, = plt.plot(oes_history, color=\"orange\", linewidth=1.0, linestyle=\"-\", label='OpenAI-ES')\n",
        "#pepg_line, = plt.plot(pepg_history, color=\"blue\", linewidth=1.0, linestyle=\"-\", label='PEPG / NES')\n",
        "#cma_line, = plt.plot(cma_history, color=\"red\", linewidth=1.0, linestyle=\"-\", label='CMA-ES')\n",
        "\n",
        "#plt.legend(handles=[optimum_line, ga_line, cma_line, pepg_line, oes_line], loc=4)\n",
        "plt.legend(handles=[optimum_line, ga_line], loc=1)\n",
        "\n",
        "\n",
        "# Set x limits\n",
        "plt.xlim(0,2500)\n",
        "\n",
        "plt.xlabel('generation')\n",
        "plt.ylabel('fitness')\n",
        "\n",
        "# plt.savefig(\"./rastrigin_10d.svg\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}